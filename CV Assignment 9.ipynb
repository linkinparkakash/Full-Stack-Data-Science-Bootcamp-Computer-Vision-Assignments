{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffdf1f0-5803-4313-9494-fc6c25eb636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What are the advantages of a CNN for image classification over a completely linked DNN?\n",
    "\n",
    "# Ans:\n",
    "# Local connectivity: CNNs exploit the spatial structure of images by using local connectivity patterns through convolutional \n",
    "# layers, allowing them to capture local patterns efficiently. Completely linked DNNs lack this specialized structure.\n",
    "\n",
    "# Parameter sharing: CNNs share weights through convolutional filters, reducing the total number of parameters. This sharing enables \n",
    "# CNNs to generalize better to new, unseen images and handle variations in position, scale, and orientation.\n",
    "\n",
    "# Translation invariance: CNNs are inherently translation invariant because the same filters are applied across different image regions. \n",
    "# This property makes them robust to small shifts or translations in the input image, improving classification accuracy.\n",
    "\n",
    "# Hierarchical feature extraction: CNNs typically have multiple layers with increasing abstraction levels. Lower layers capture low-level\n",
    "# features like edges and textures, while higher layers capture more complex and semantic features. This hierarchical feature extraction \n",
    "# helps CNNs learn meaningful representations for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04613586-4023-448d-9e55-e44c9e0475f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two,\n",
    "# and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the\n",
    "# top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does\n",
    "# the CNN have in total? How much RAM would this network need when making a single instance\n",
    "# prediction if we&#39;re using 32-bit floats? What if you were to practice on a batch of 50 images?\n",
    "\n",
    "# Ans:\n",
    "# The CNN described has a total of 3 + 3 + 3 = 9 convolutional kernels or filters. Since each layer generates a different number of \n",
    "# feature maps, the total number of feature maps is 100 + 200 + 400 = 700.\n",
    "\n",
    "# To calculate the RAM needed for a single instance prediction, we need to consider the size of each feature map. Assuming 32-bit \n",
    "# floats are used, each pixel in a feature map occupies 4 bytes. The input image size is 200 x 300 pixels, and with SAME padding and \n",
    "# stride 2 in each convolutional layer, the feature map sizes are reduced by a factor of 2 in each dimension. Therefore, the RAM needed\n",
    "# for a single instance prediction is:\n",
    "\n",
    "# 200 x 300 x 4 bytes (input image) + 100 x (200/2) x (300/2) x 4 bytes (bottom layer feature maps) +\n",
    "# 200 x (200/4) x (300/4) x 4 bytes (middle layer feature maps) +\n",
    "# 400 x (200/8) x (300/8) x 4 bytes (top layer feature maps) = Total RAM usage in bytes.\n",
    "\n",
    "# If practicing on a batch of 50 images, the RAM needed would be 50 times the RAM usage calculated for a single instance prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980fbf3a-24ab-41e7-8f56-de9b1c2f4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What are five things you might do to fix the problem if your GPU runs out of memory while training a CNN?\n",
    "\n",
    "# Ans:\n",
    "# Reduce batch size: Decrease the number of samples processed in each batch, which reduces the memory requirement per batch.\n",
    "\n",
    "# Use mixed precision training: Utilize lower precision (e.g., float16) for storing weights and activations, which reduces memory usage\n",
    "# without significantly impacting model performance.\n",
    "\n",
    "# Apply gradient checkpointing: Trade-off computation for memory by recomputing intermediate activations during backpropagation, \n",
    "# reducing memory consumption.\n",
    "\n",
    "# Limit model complexity: Reduce the number of layers, neurons, or parameters in the CNN architecture to decrease memory requirements.\n",
    "\n",
    "# Utilize model parallelism: Split the model across multiple GPUs, distributing the memory load and allowing for larger models to fit \n",
    "# within the combined memory capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12658c2-4e3c-4f21-9a6e-643183050cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?\n",
    "\n",
    "# Ans:\n",
    "# Dimensionality reduction: Max pooling reduces the spatial dimensions of the feature maps, resulting in a smaller output size.\n",
    "# This can help to reduce memory requirements and computational complexity in subsequent layers.\n",
    "\n",
    "# Translation invariance: Max pooling introduces a degree of translation invariance by capturing the maximum value within each pooling \n",
    "# region. This allows the network to focus on the most prominent features while being less sensitive to slight spatial shifts or \n",
    "# variations in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8906b7f9-e5a6-4126-a1fd-5ea512ac20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. When would a local response normalization layer be useful?\n",
    "\n",
    "# Ans:\n",
    "# A local response normalization layer can be useful in CNN architectures when there is a need for local contrast normalization or lateral \n",
    "# inhibition. It helps to enhance the activation of neurons that are relatively more active compared to their neighboring neurons, \n",
    "# promoting competition and improving the model's ability to capture salient features. Local response normalization can be particularly \n",
    "# beneficial in tasks such as object detection, where precise localization and discrimination of objects are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365bbd25-c432-4788-b830-eba14940c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and\n",
    "# ResNet&#39;s core innovations?\n",
    "\n",
    "# Ans:\n",
    "# AlexNet: Increased depth, ReLU activation, local response normalization.\n",
    "\n",
    "# GoogLeNet: Inception modules for multi-scale feature extraction, reducing parameters.\n",
    "\n",
    "# ResNet: Skip connections for training very deep networks, learning residual mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a01a41-3d72-489a-b39d-4432f3d4b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. On MNIST, build your own CNN and strive to achieve the best possible accuracy.\n",
    "\n",
    "# Ans:\n",
    "# Input layer: Accepts grayscale images of size 28x28.\n",
    "# Convolutional layer: Applies 32 filters of size 3x3, using ReLU activation.\n",
    "# Max pooling layer: Performs max pooling with a pool size of 2x2.\n",
    "# Convolutional layer: Applies 64 filters of size 3x3, using ReLU activation.\n",
    "# Max pooling layer: Performs max pooling with a pool size of 2x2.\n",
    "# Flatten layer: Flattens the 2D feature maps into a 1D vector.\n",
    "# Fully connected layer: Consists of 128 neurons, using ReLU activation.\n",
    "# Dropout layer: Helps prevent overfitting by randomly dropping out neurons during training.\n",
    "# Output layer: Consists of 10 neurons with softmax activation for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c46d1-002c-4335-9dd6-17d9a3474fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Using Inception v3 to classify broad images. a.\n",
    "# Images of different animals can be downloaded. Load them in Python using the\n",
    "# matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop\n",
    "# them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency.\n",
    "# The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to 1.0, so make sure yours do as well.\n",
    "\n",
    "# Sol:\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# List of image URLs\n",
    "image_urls = [\n",
    "    'https://thumbs.dreamstime.com/b/beautiful-rain-forest-ang-ka-nature-trail-36703721.jpg',\n",
    "    'https://images.unsplash.com/photo-1503023345310-bd7c1de61c7d?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxzZWFyY2h8Mnx8aHVtYW58ZW58MHx8MHx8&w=1000&q=80',\n",
    "\n",
    "]\n",
    "\n",
    "# Load, resize, crop, and preprocess images\n",
    "preprocessed_images = []\n",
    "\n",
    "for url in image_urls:\n",
    "    # Load the image from URL\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        image_data = f.read()\n",
    "\n",
    "    # Convert image data to PIL Image object\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "    # Resize and crop the image to 299x299 pixels\n",
    "    image = image.resize((299, 299), Image.BILINEAR)\n",
    "\n",
    "    # Ensure the image has three channels (RGB)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Convert the image to a numpy array and preprocess it\n",
    "    image_array = np.array(image)\n",
    "    preprocessed_image = (image_array / 255.0) * 2.0 - 1.0\n",
    "\n",
    "    # Add preprocessed image to the list\n",
    "    preprocessed_images.append(preprocessed_image)\n",
    "\n",
    "# Now, the preprocessed_images list contains the preprocessed images ready for classification using Inception v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea0a686-8694-40f4-888f-80b54b16e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Large-scale image recognition using transfer learning.\n",
    "# a. Make a training set of at least 100 images for each class. You might, for example, identify your\n",
    "# own photos based on their position (beach, mountain, area, etc.) or use an existing dataset, such as\n",
    "# the flowers dataset or MIT&#39;s places dataset (requires registration, and it is huge).\n",
    "# b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also\n",
    "# adding some randomness for data augmentation.\n",
    "# c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer (the\n",
    "# last layer before output layer) and replace output layer with appropriate number of outputs for\n",
    "# your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the\n",
    "# output layer must have five neurons and use softmax activation function).\n",
    "# d. Separate the data into two sets: a training and a test set. The training set is used to train the\n",
    "# model, and the test set is used to evaluate it.\n",
    "\n",
    "# Sol:\n",
    "\n",
    "# To perform large-scale image recognition using transfer learning with Inception v3:\n",
    "# a. Create a training set with at least 100 images per class.\n",
    "# b. Preprocess the images by resizing, cropping, and applying data augmentation.\n",
    "# c. Modify the Inception v3 model by freezing layers and replacing the output layer.\n",
    "# d. Split the data into training and test sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e12dd-c614-4462-9738-7bd53a78224e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
