{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec559b07-4d38-4dc6-8d59-253191dacc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?\n",
    "\n",
    "# Ans:\n",
    "# Trainable parameters, also known as learnable parameters, are the set of weights and biases in a neural network that are adjusted \n",
    "# during the training process through backpropagation and optimization algorithms. These parameters are updated iteratively to minimize \n",
    "# the difference between the predicted output and the actual output during training. They are learned from the data and directly\n",
    "# influence the behavior and performance of the model.\n",
    "\n",
    "# Non-trainable parameters, also known as fixed parameters, are not updated during the training process. These parameters are usually\n",
    "# pre-defined or set externally and remain constant throughout the training. They are not adjusted based on the training data and do\n",
    "# not contribute to the learning process. Examples of non-trainable parameters include hyperparameters such as the learning rate, batch\n",
    "# size, activation functions, network architecture (e.g., number of layers), and other settings that are determined before training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5671ac09-6ece-4af0-96d0-21e885ea82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. In the CNN architecture, where does the DROPOUT LAYER go?\n",
    "\n",
    "# Ans:\n",
    "# In the CNN architecture, the dropout layer is typically placed after one or more convolutional layers or fully connected layers.\n",
    "# Its purpose is to randomly drop out a fraction of the neurons in the previous layer during training, which helps prevent overfitting \n",
    "# and improves the generalization ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11df9b06-a673-4c09-8035-cf99b4ec137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What is the optimal number of hidden layers to stack?\n",
    "\n",
    "# Ans:\n",
    "# The optimal number of hidden layers to stack in a neural network depends on the complexity of the problem at hand and the available data.\n",
    "# There is no one-size-fits-all answer, and it is often determined through experimentation and model validation. However, for many tasks, \n",
    "# a few hidden layers (e.g., 1-3) are sufficient to capture the necessary representations and patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ab06a0-bed8-4602-b3bf-f1b4bd55cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. In each layer, how many secret units or filters should there be?\n",
    "\n",
    "# Ans:\n",
    "# The number of neurons or filters in each layer of a neural network, including convolutional layers, depends on the complexity of the\n",
    "# problem, the size of the input data, and the available computational resources. There is no fixed optimal number, but it is often \n",
    "# recommended to start with a relatively small number and gradually increase it if the model underfits. Experimentation and validation \n",
    "# are crucial to determine the appropriate number of units or filters for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441185f5-4628-41e8-99c3-544716b2fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What should your initial learning rate be?\n",
    "\n",
    "# Ans:\n",
    "# The choice of initial learning rate depends on various factors such as the problem, dataset, and network architecture. Typically, a\n",
    "# commonly used initial learning rate is 0.1, but it can vary significantly depending on the specific scenario. It is generally \n",
    "# recommended to start with a moderate initial learning rate and adjust it during training using techniques like learning rate decay \n",
    "# or adaptive learning rate methods to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7c5cb5-f1a5-4138-94b6-f24a8f3bf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. What do you do with the activation function?\n",
    "\n",
    "# Ans:\n",
    "# The activation function introduces non-linearity to the output of a neuron in a neural network. It helps the model learn complex \n",
    "# patterns and relationships in the data by enabling the network to approximate non-linear functions. The activation function is applied \n",
    "# element-wise to the output of each neuron in a layer, transforming the input to the desired range and determining whether the neuron \n",
    "# should be activated or not based on a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba697d1d-3ab2-425a-9da1-fe0754660fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. What is NORMALIZATION OF DATA?\n",
    "\n",
    "# Ans:\n",
    "# Normalization of data is the process of scaling and transforming the input features to a standard range or distribution. It aims to \n",
    "# bring the data to a similar scale, preventing certain features from dominating others due to their larger magnitude. Common normalization\n",
    "# techniques include methods such as min-max scaling, z-score standardization, and feature scaling, which ensure that the data falls\n",
    "# within a specific range or has a zero mean and unit variance. Normalizing data can improve the performance and convergence of machine\n",
    "# learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea4bc66-2930-4d32-9743-6c354ae55a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What is IMAGE AUGMENTATION and how does it work?\n",
    "\n",
    "# Ans:\n",
    "# Image augmentation is a technique used in computer vision tasks to artificially expand a training dataset by applying various \n",
    "# transformations to the original images. It helps improve the model's ability to generalize and handle variations in the input data.\n",
    "# Image augmentation techniques include rotation, flipping, scaling, cropping, translation, and introducing noise or distortions to the\n",
    "# images. These transformations create new variations of the original images, increasing the diversity of the training data and reducing \n",
    "# overfitting. By randomly applying these transformations during training, the model learns to be more robust and adaptable to different \n",
    "# real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aba6a99-101f-44d9-9090-ec1e7efe492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. What is DECLINE IN LEARNING RATE?\n",
    "\n",
    "# What does EARLY STOPPING CRITERIA mean?\n",
    "\n",
    "# Ans:\n",
    "# Decline in learning rate refers to the reduction or decay of the learning rate during the training process of a neural network. \n",
    "# It is commonly employed to improve convergence and fine-tuning of the model. By gradually decreasing the learning rate over time,\n",
    "# the optimization algorithm takes smaller steps towards the minimum of the loss function, allowing for more precise updates and avoiding\n",
    "# overshooting.\n",
    "\n",
    "# Early stopping criteria refers to a technique used to prevent overfitting and determine when to stop the training process. \n",
    "# It involves monitoring a validation metric (e.g., validation loss or accuracy) during training and stopping the training if the \n",
    "# performance on the validation set starts to deteriorate or no longer improves significantly. By stopping the training early, it \n",
    "# helps avoid overfitting and saves computational resources by preventing unnecessary iterations that may lead to degraded generalization\n",
    "# performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39803207-f902-4a30-b574-36050820a55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
