{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006e3990-3b30-4a34-9e14-70054b9e02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How can each of these parameters be fine-tuned? • Number of hidden layers\n",
    "# • Network architecture (network depth)\n",
    "\n",
    "# • Each layer&#39;s number of neurons (layer width)\n",
    "\n",
    "# • Form of activation\n",
    "\n",
    "# • Optimization and learning\n",
    "\n",
    "# • Learning rate and decay schedule\n",
    "\n",
    "# • Mini batch size\n",
    "\n",
    "# • Algorithms for optimization\n",
    "\n",
    "# • The number of epochs (and early stopping criteria)\n",
    "\n",
    "# • Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "# • L2 normalization\n",
    "\n",
    "# • Drop out layers\n",
    "# • Data augmentation\n",
    "\n",
    "# Ans:\n",
    "# Number of hidden layers: It can be fine-tuned by adjusting the number of layers in the neural network.\n",
    "\n",
    "# Network architecture: The network depth can be fine-tuned by modifying the overall structure and arrangement of layers in the neural\n",
    "# network.\n",
    "\n",
    "# Layer width: Each layer's number of neurons can be fine-tuned by adjusting the number of neurons in each layer of the neural network.\n",
    "\n",
    "# Activation function: The form of activation can be fine-tuned by selecting different activation functions for each layer in the \n",
    "# neural network.\n",
    "\n",
    "# Optimization and learning: Different optimization algorithms and learning techniques can be employed to fine-tune the neural network's\n",
    "# performance.\n",
    "\n",
    "# Learning rate and decay schedule: The learning rate and its decay schedule can be fine-tuned by adjusting the magnitude and rate of \n",
    "# learning rate decay during training.\n",
    "\n",
    "# Mini-batch size: The mini-batch size can be fine-tuned by selecting an appropriate number of samples to be processed together in each \n",
    "# training iteration.\n",
    "\n",
    "# Optimization algorithms: Various optimization algorithms, such as stochastic gradient descent (SGD) or Adam, can be used to fine-tune\n",
    "# the neural network.\n",
    "\n",
    "# Number of epochs: The number of epochs can be fine-tuned by specifying the number of complete passes through the training dataset \n",
    "# during training, and early stopping criteria can be employed to determine when to stop training.\n",
    "\n",
    "# Regularization techniques: Techniques such as L2 normalization can be used to prevent overfitting and improve generalization of the \n",
    "# neural network.\n",
    "\n",
    "# Dropout layers: Dropout layers can be added to the neural network to reduce overfitting by randomly dropping out a portion of neurons\n",
    "# during training.\n",
    "\n",
    "# Data augmentation: Data augmentation techniques can be applied to increase the size and diversity of the training dataset, which \n",
    "# helps improve the neural network's ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad249865-e239-40f2-a769-2c8db6b100be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
