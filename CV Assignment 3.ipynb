{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63ce6a3-1a7a-4c35-b3bc-5f5d662ea361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. After each stride-2 conv, why do we double the number of filters?\n",
    "\n",
    "# Ans:\n",
    "# Doubling the number of filters after each stride-2 convolution helps to capture and represent more complex and higher-level features \n",
    "# in the network. The increased number of filters allows the network to learn a richer set of patterns and enables it to extract more \n",
    "# diverse and detailed information from the input data. This can lead to better overall performance and improved representation of \n",
    "# the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7419d107-bcb0-4689-8da6-a6b5fbc09882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
    "\n",
    "# Ans:\n",
    "# Using a larger kernel in the first convolutional layer of the MNIST CNN allows the network to capture larger spatial patterns and \n",
    "# features in the input images. Since MNIST digits are relatively small and contain simple shapes, using a larger kernel helps to extract\n",
    "# more informative and discriminative features from the input data. It allows the network to learn more complex representations and\n",
    "# improve its ability to distinguish between different digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b51b4f-1032-41bb-9077-114e7e4927ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What data is saved by ActivationStats for each layer?\n",
    "\n",
    "# Ans:\n",
    "# The ActivationStats module saves statistics related to the activations of each layer in a neural network. It typically includes \n",
    "# information such as the mean, standard deviation, minimum, and maximum values of the activations. These statistics provide insights \n",
    "# into the distribution and magnitude of the activations, which can be helpful for understanding the behavior and performance of the \n",
    "# network during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f633f0eb-1351-448a-90fb-ab1c234f9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How do we get a learner&#39;s callback after they&#39;ve completed training?\n",
    "\n",
    "# Ans:\n",
    "# To get a learner's callback after they've completed training, you can use the after_fit callback. This callback is triggered once the \n",
    "# training is complete and allows you to perform any additional actions or computations on the learner or its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289dbbf1-42ca-4ece-a6a0-6d502865ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. What are the drawbacks of activations above zero?\n",
    "\n",
    "# Ans:\n",
    "# One drawback of activations above zero is that they can lead to exploding gradients during the training process. This can make the \n",
    "# optimization process unstable and hinder the convergence of the neural network. Additionally, activations above zero can also introduce \n",
    "# non-linearity in the network, which may not always be desired depending on the specific task or problem being addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e06931a-6c62-42ad-adaf-66606d34ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "\n",
    "# Ans:\n",
    "# Benefits of using larger batches:\n",
    "\n",
    "# Improved training efficiency by leveraging parallelism in hardware.\n",
    "# Smoother optimization process with reduced noise in gradient estimation.\n",
    "# Allows for better utilization of computational resources.\n",
    "# Drawbacks of using larger batches:\n",
    "\n",
    "# Higher memory requirements due to storing larger batches of data.\n",
    "# Increased difficulty in finding good learning rate schedules.\n",
    "# Limited generalization performance, as larger batches may lead to suboptimal solutions compared to using smaller batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d04695-a176-4819-a10d-e85d124f17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Why should we avoid starting training with a high learning rate?\n",
    "\n",
    "# Ans:\n",
    "# Starting training with a high learning rate can lead to unstable and erratic training behavior, making it difficult for the model to\n",
    "# converge to an optimal solution. It may cause the loss to initially decrease rapidly but then fluctuate or diverge later on. \n",
    "# Therefore, it is advisable to start with a lower learning rate and gradually increase it during training to ensure a more stable \n",
    "# and effective optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a333760-b17d-4f86-9cd6-d98f1415a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. What are the pros of studying with a high rate of learning?\n",
    "\n",
    "# Ans:\n",
    "# Studying with a high learning rate can lead to faster initial progress during training. It allows the model to quickly explore the \n",
    "# parameter space and make significant updates to its weights. This can be beneficial in situations where there is a time constraint or \n",
    "# when dealing with simple and well-behaved optimization problems. However, it also increases the risk of overshooting the optimal \n",
    "# solution and may result in unstable training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047a4489-a1bb-4108-8154-4b8bfeca383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Why do we want to end the training with a low learning rate?\n",
    "\n",
    "# Ans:\n",
    "# Ending the training with a low learning rate allows the model to make smaller and more precise updates to its weights,\n",
    "# leading to fine-tuning and convergence towards the optimal solution. A low learning rate helps to stabilize the training process,\n",
    "# reduce the chances of overshooting the optimal solution, and improve the model's generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb5e77-8ebf-4fb9-955a-ab608e55d8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
